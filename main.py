import streamlit as st
try:
    from openai.error import OpenAIError
except ImportError:
    OpenAIError = Exception  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø¹Ø§Ù… Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ OpenAIError
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI
#from langchain.chat_models import ChatOpenAI
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

# Set up ChatOpenAI
chat = ChatOpenAI(temperature=0.5)

# Style setup
with open("style.css") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

st.title("ğŸ¥°ğŸ¥°ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­ Ø£ÙŠ Ø³Ø¤Ø§Ù„ ÙˆØ³Ø£Ø¬ÙŠØ¨ Ø¹Ù†Ù‡")

# Initialize conversation in session state
if 'flowmessages' not in st.session_state:
    st.session_state['flowmessages'] = [
        SystemMessage(content="Ø£Ù†Øª Ù…Ø¹Ù„Ù… Ø®Ø¨ÙŠØ± ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø°ÙƒØ±Ø§Øª Ø¨ÙŠØ¯Ø§ØºÙˆØ¬ÙŠØ© ÙˆØ§Ù†ØªØ§Ø¬ ØªÙ…Ø±ÙŠÙ†Ø§Øª ÙˆÙ…Ø³Ø§Ø¦Ù„ Ù†Ù…ÙˆØ°Ø¬ÙŠØ©")
    ]

conversation = st.session_state['flowmessages']

# Function to load OpenAI model and get response
def get_chatmodel_response(question):
    try:
        conversation.append(HumanMessage(content=question))
        answer = chat(conversation)
        conversation.append(AIMessage(content=answer.content))
        return answer.content
    except OpenAIError as e:
        if 'rate limits' in str(e):
            return "Ù†Ø¹ØªØ°Ø±ØŒ Ù„Ù‚Ø¯ ØªØ¬Ø§ÙˆØ²Ù†Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…. Ø³ÙŠØ¹ÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„Ø¹Ù…Ù„ Ù‚Ø±ÙŠØ¨Ù‹Ø§ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§."
        else:
            return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù…: {str(e)}"

# Display chat history
for message in conversation[1:]:
    if isinstance(message, HumanMessage):
        with st.chat_message("user", avatar="ğŸ¤”"):
            st.write(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.write(message.content)

# Input field for user query
prompt = st.chat_input("Ø£Ø±Ø¬ÙˆÙƒ Ø­Ø§ÙˆÙ„ Ø£Ù† ØªØ¬Ø¹Ù„ Ø³Ø¤Ø§Ù„Ùƒ ÙˆØ§Ø¶Ø­Ø§ Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù†")
if prompt:
    # Display user message in chat message container
    with st.chat_message("user", avatar="ğŸ¤”"):
        st.markdown(prompt)
    # Get AI response
    response = get_chatmodel_response(prompt)
    with st.chat_message("assistant"):
        st.markdown(response)
